{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f057c3b6",
   "metadata": {},
   "source": [
    "# Projet 2 - Fashion Trend Intelligence | Segmentation vestimentaire avec IA\n",
    "\n",
    "Ce notebook permet d‚Äô√©valuer la faisabilit√© technique du mod√®le SegFormer-clothes, afin de d√©terminer s‚Äôil est capable d‚Äôidentifier et d‚Äôisoler avec pr√©cision chaque pi√®ce vestimentaire pr√©sente dans une image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2e7c8a",
   "metadata": {},
   "source": [
    "## 1. Installation du projet et de son environnement\n",
    "\n",
    "Afin d‚Äôutiliser correctement ce notebook, v√©rifiez que vous disposez du bon environnement pour l‚Äôex√©cuter.\n",
    "\n",
    "\n",
    "### 1.1 Installation de Python\n",
    "\n",
    "Pour ce projet, il est n√©cessaire d‚Äôavoir **au minimum Python 3.8**.  \n",
    "Si ce n‚Äôest pas d√©j√† le cas, vous pouvez vous r√©f√©rer √† [la documentation officielle](https://www.python.org/downloads/).\n",
    "\n",
    "V√©rifiez votre version de Python :\n",
    "\n",
    "```bash\n",
    "python --version\n",
    "```\n",
    "\n",
    "### 1.2 Installation de `uv`\n",
    "\n",
    "`uv` est un gestionnaire de projets Python permettant d‚Äôinstaller et d‚Äôorganiser les d√©pendances plus rapidement et plus simplement que les outils traditionnels (`pip`, `virtualenv`, etc.).\n",
    "\n",
    "Pour installer `uv`, veuillez suivre [la documentation officielle](https://docs.astral.sh/uv/getting-started/installation/#standalone-installer)\n",
    "\n",
    "V√©rifiez l‚Äôinstallation :\n",
    "\n",
    "```bash\n",
    "uv --version\n",
    "```\n",
    "\n",
    "### 1.3 Cr√©ation du projet\n",
    "\n",
    "Cr√©ez un nouveau projet Python avec `uv` :\n",
    "\n",
    "```bash\n",
    "uv init nom_du_projet\n",
    "cd nom_du_projet\n",
    "```\n",
    "\n",
    "La structure de base du projet est alors g√©n√©r√©e automatiquement.\n",
    "\n",
    "\n",
    "### 1.4 Cr√©ation et activation de l‚Äôenvironnement virtuel\n",
    "\n",
    "Cr√©ez l‚Äôenvironnement virtuel :\n",
    "\n",
    "```bash\n",
    "uv venv\n",
    "```\n",
    "\n",
    "Activez-le selon votre syst√®me :\n",
    "\n",
    "* **Linux / macOS**\n",
    "\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "* **Windows (PowerShell)**\n",
    "\n",
    "```powershell\n",
    ".venv\\Scripts\\Activate.ps1\n",
    "```\n",
    "\n",
    "Voir la documentation officielle :\n",
    "[https://docs.astral.sh/uv/pip/environments/#creating-a-virtual-environment](https://docs.astral.sh/uv/pip/environments/#creating-a-virtual-environment)\n",
    "\n",
    "\n",
    "\n",
    "### 1.5 Installation des d√©pendances\n",
    "\n",
    "> ‚ùó Assurez-vous que l‚Äôenvironnement virtuel est **activ√©** avant d‚Äôinstaller les d√©pendances.\n",
    "\n",
    "Installez les biblioth√®ques n√©cessaires au projet :\n",
    "\n",
    "```bash\n",
    "uv add ipykernel jupyterlab requests pillow matplotlib numpy tqdm python-dotenv aiohttp\n",
    "```\n",
    "\n",
    "Ces d√©pendances sont automatiquement enregistr√©es dans le fichier `pyproject.toml`.\n",
    "\n",
    "Vous pouvez visualiser l‚Äôensemble des d√©pendances install√©es avec la commande suivante :\n",
    "\n",
    "```bash\n",
    "uv tree\n",
    "```\n",
    "\n",
    "### 1.6 Cr√©ation du fichier `.env`\n",
    "\n",
    "Cr√©ez un fichier nomm√© `.env` √† la racine du projet.\n",
    "\n",
    "Copiez-y le contenu suivant et remplacez les valeurs si n√©cessaire :\n",
    "\n",
    "```env\n",
    "# Token d'authentification √† l'API Hugging Face\n",
    "HF_API_TOKEN=VOTRE_TOKEN_HUGGING_FACE_ICI\n",
    "\n",
    "# Chemins du dataset\n",
    "DATASET_IMAGES_DIR=content/images_a_segmenter\n",
    "DATASET_ANNOTATIONS_DIR=content/annotations\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Important** :\n",
    "\n",
    "* Ne partagez jamais votre fichier `.env`\n",
    "* Ajoutez-le √† votre `.gitignore`\n",
    "\n",
    "```gitignore\n",
    ".env\n",
    "```\n",
    "\n",
    "### 1.7 Cr√©ation d‚Äôun token Hugging Face\n",
    "\n",
    "1. Cr√©ez un compte sur [https://huggingface.co/](https://huggingface.co/)\n",
    "2. Allez dans **Profile ‚Üí Settings ‚Üí Access Tokens**\n",
    "3. Cr√©ez un nouveau token (r√¥le **read** suffisant)\n",
    "4. Copiez le token dans la variable `HF_API_TOKEN` du fichier `.env`\n",
    "\n",
    "\n",
    "### 1.8 Pr√©paration des jeux de donn√©es\n",
    "\n",
    "R√©cup√©rez les jeux de donn√©es (images et annotations) et placez-les dans les dossiers suivants :\n",
    "\n",
    "```\n",
    "content/\n",
    "‚îú‚îÄ‚îÄ images_a_segmenter/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ image_0.jpg\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ image_1.jpg\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îî‚îÄ‚îÄ annotations/\n",
    "    ‚îú‚îÄ‚îÄ mask_0.json\n",
    "    ‚îú‚îÄ‚îÄ mask_1.json\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87239d3",
   "metadata": {},
   "source": [
    "## 2. Importation des Biblioth√®ques\n",
    "\n",
    "Commen√ßons par importer les biblioth√®ques Python n√©cessaires. Nous aurons besoin de :\n",
    "- **`os`** : Interaction avec le syst√®me de fichiers (navigation dans les r√©pertoires, listage des fichiers images).\n",
    "- **`requests`** : Envoi de requ√™tes HTTP vers l'API pour la segmentation d'images.\n",
    "- **`PIL (Pillow)`** : Manipulation et traitement des images (ouverture, redimensionnement, conversion).\n",
    "- **`matplotlib.pyplot`** : Visualisation des images originales et des masques de segmentation.\n",
    "- **`matplotlib.patches`** : Cr√©ation d'√©l√©ments graphiques personnalis√©s pour les l√©gendes des visualisations.\n",
    "- **`numpy`** : Manipulation efficace des tableaux num√©riques repr√©sentant les pixels des images.\n",
    "- **`tqdm.notebook`** : Affichage d'une barre de progression interactive dans les notebooks Jupyter (utile lors du traitement par lot).\n",
    "- **`base64`** : Encodage/d√©codage en Base64 des images et masques √©chang√©s avec l'API.\n",
    "- **`io`** : Gestion des flux de donn√©es en m√©moire pour la manipulation des images sans fichiers temporaires.\n",
    "- **`python-dotenv`** : Chargement s√©curis√© des variables d'environnement (comme les cl√©s API) depuis un fichier `.env`.\n",
    "- **`time`** : Gestion des d√©lais entre les appels API pour respecter les limites de taux (rate limiting).\n",
    "- **`re`** : Traitement d'expressions r√©guli√®res (parsing de r√©ponses, validation de formats).\n",
    "- **`aiohttp`** : Requ√™tes HTTP asynchrones pour am√©liorer les performances lors du traitement par lot d'images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a41d4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "import base64\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import re\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d06005",
   "metadata": {},
   "source": [
    "## 3. Chargement de la Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b649df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dossier 'content/images_a_segmenter' existant.\n",
      "‚úì Dossier 'content/annotations' existant.\n"
     ]
    }
   ],
   "source": [
    "# Charger les variables d'environnement depuis le fichier .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# R√©cup√©ration des variables d'environnement\n",
    "IMAGES_DIR = os.getenv('DATASET_IMAGES_DIR')\n",
    "ANNOTATIONS_DIR = os.getenv('DATASET_ANNOTATIONS_DIR')\n",
    "API_TOKEN = os.getenv('HF_API_TOKEN')\n",
    "MAX_IMAGES = 3  # Nombre maximum d'images √† traiter\n",
    "\n",
    "# V√©rification de la pr√©sence des variables\n",
    "if not IMAGES_DIR or not API_TOKEN or not ANNOTATIONS_DIR:\n",
    "    raise ValueError(\"Variables d'environnement manquantes dans le fichier .env\")\n",
    "\n",
    "# V√©rification du token\n",
    "if API_TOKEN == \"VOTRE_TOKEN_HUGGING_FACE_ICI\":\n",
    "    raise ValueError(\"Vous devez remplacer 'VOTRE_TOKEN_HUGGING_FACE_ICI' par votre token API personnel dans le fichier .env\")\n",
    "\n",
    "# Cr√©ation des dossiers s'ils n'existent pas\n",
    "for directory, name in [(IMAGES_DIR, \"images\"), (ANNOTATIONS_DIR, \"annotations\")]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"‚úì Dossier '{directory}' cr√©√© pour les {name}.\")\n",
    "    else:\n",
    "        print(f\"‚úì Dossier '{directory}' existant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095821a",
   "metadata": {},
   "source": [
    "## 4. Configuration de l'API Hugging Face\n",
    "\n",
    "### 4.1 Pr√©sentation du mod√®le\n",
    "Nous utilisons le mod√®le [**SegFormer-B3-Clothes**](https://huggingface.co/sayeed99/segformer_b3_clothes) qui est sp√©cialis√© dans la segmentation s√©mantique de v√™tements. Ce mod√®le peut d√©tecter 18 classes diff√©rentes :\n",
    "- √âl√©ments vestimentaires : chapeau, haut, jupe, pantalon, robe, ceinture, chaussures, sac, √©charpe\n",
    "- Parties du corps : cheveux, visage, bras, jambes\n",
    "- Accessoires : lunettes de soleil\n",
    "- Arri√®re-plan\n",
    "\n",
    "### 4.2 Configuration de l'endpoint API\n",
    "L'API Hugging Face Inference Router permet d'interroger le mod√®le sans avoir √† le d√©ployer localement. Voici les √©l√©ments de configuration :\n",
    "\n",
    "**URL de l'API** : `https://router.huggingface.co/hf-inference/models/sayeed99/segformer_b3_clothes`\n",
    "\n",
    "**Headers requis** :\n",
    "- `Authorization: Bearer YOUR_TOKEN` : Authentification avec votre token personnel\n",
    "- `Content-Type: image/jpeg` ou `image/png` : Type MIME de l'image envoy√©e\n",
    "\n",
    "**Format de r√©ponse** : L'API retourne un JSON contenant une liste de masques encod√©s en base64, un par classe d√©tect√©e.\n",
    "\n",
    "**S√©curit√©** : Ne jamais hardcoder votre token dans le notebook. Elle doit mis dans la variable d'environnement `HF_API_TOKEN'` le fichier `.env`\n",
    "\n",
    "### 4.3 Bonnes pratiques d'utilisation\n",
    "1. **Rate Limiting** : Espacer les requ√™tes de 1-2 secondes pour √©viter les erreurs 429 (Too Many Requests)\n",
    "2. **Timeout** : D√©finir un timeout de 30 secondes minimum (le mod√®le peut √™tre lent)\n",
    "3. **Gestion d'erreurs** : Pr√©voir les cas suivants :\n",
    "   - 401 : Token invalide\n",
    "   - 429 : Trop de requ√™tes\n",
    "   - 500 : Erreur serveur (r√©essayer apr√®s quelques secondes)\n",
    "   - 503 : Mod√®le en cours de chargement (attendre et r√©essayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://router.huggingface.co/hf-inference/models/sayeed99/segformer_b3_clothes\"\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\"\n",
    "    # Le \"Content-Type\" sera ajout√© dynamiquement lors de l'envoi de l'image\n",
    "}\n",
    "\n",
    "\n",
    "# Rate limiting pour compte gratuit (d'apr√®s la doc HF)\n",
    "# Inference API : ~1000 requ√™tes/jour, avec burst de ~100 requ√™tes/min\n",
    "REQUESTS_PER_MINUTE = 30  # On reste prudent (bien en dessous de 100)\n",
    "DELAY_BETWEEN_REQUESTS = 60 / REQUESTS_PER_MINUTE  # ~2 secondes entre chaque requ√™te\n",
    "\n",
    "\n",
    "# Configuration des requ√™tes asynchrones\n",
    "MAX_CONCURRENT_REQUESTS = 5      # Nombre de requ√™tes simultan√©es maximum\n",
    "MAX_RETRIES = 3                  # Nombre de tentatives en cas d'erreur\n",
    "INITIAL_RETRY_DELAY = 2          # D√©lai initial entre les tentatives (secondes)\n",
    "REQUEST_TIMEOUT = 30             # Timeout des requ√™tes (secondes)\n",
    "\n",
    "async def segment_clothes_async(\n",
    "    session: aiohttp.ClientSession,\n",
    "    single_image_path: str,\n",
    "    semaphore: asyncio.Semaphore\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Requ√™te asynchrone √† l'API Hugging Face avec rate limiting.\n",
    "\n",
    "    Args:\n",
    "        session: Session aiohttp partag√©e\n",
    "        single_image_path: Chemin vers l'image √† segmenter\n",
    "        semaphore: S√©maphore pour limiter les requ√™tes concurrentes\n",
    "\n",
    "    Returns:\n",
    "        dict: R√©sultats de la segmentation (JSON de l'API)\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: Si le fichier image n'existe pas\n",
    "        ValueError: Si le format d'image n'est pas support√©\n",
    "        aiohttp.ClientError: Si la requ√™te API √©choue\n",
    "    \"\"\"\n",
    "    \n",
    "    # V√©rification de l'existence du fichier\n",
    "    if not os.path.exists(single_image_path):\n",
    "        raise FileNotFoundError(f\"L'image '{single_image_path}' n'existe pas\")\n",
    "    \n",
    "    # V√©rification du format d'image\n",
    "    try:\n",
    "        with Image.open(single_image_path) as img:\n",
    "            image_format = img.format\n",
    "            \n",
    "            if image_format not in ['JPEG', 'PNG']:\n",
    "                raise ValueError(\n",
    "                    f\"Format '{image_format}' non support√©. \"\n",
    "                    f\"Formats accept√©s: JPEG, PNG\"\n",
    "                )\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Impossible d'ouvrir l'image: {e}\")\n",
    "    \n",
    "    # Lecture du contenu binaire\n",
    "    try:\n",
    "        with open(single_image_path, 'rb') as fichier:\n",
    "            image_data = fichier.read()\n",
    "    except IOError as e:\n",
    "        raise IOError(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "    \n",
    "    # Configuration des headers\n",
    "    request_headers = headers.copy()\n",
    "    request_headers[\"Content-Type\"] = f\"image/{image_format.lower()}\"\n",
    "    \n",
    "    # Utilisation du s√©maphore pour limiter les requ√™tes concurrentes\n",
    "    async with semaphore:\n",
    "        retry_delay = INITIAL_RETRY_DELAY\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                async with session.post(\n",
    "                    API_URL,\n",
    "                    headers=request_headers,\n",
    "                    data=image_data,\n",
    "                    timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)\n",
    "                ) as response:\n",
    "                    \n",
    "                    # Gestion des diff√©rents codes de statut\n",
    "                    if response.status == 503:\n",
    "                        # Mod√®le en cours de chargement\n",
    "                        print(f\"‚è≥ Mod√®le en chargement, attente {INITIAL_RETRY_DELAY}s...\")\n",
    "                        await asyncio.sleep(INITIAL_RETRY_DELAY)\n",
    "                        retry_delay *= 2  # Backoff exponentiel\n",
    "                        continue\n",
    "                    \n",
    "                    elif response.status == 429:\n",
    "                        # Rate limit atteint\n",
    "                        retry_after = response.headers.get('Retry-After', retry_delay)\n",
    "                        wait_time = int(retry_after) if isinstance(retry_after, str) and retry_after.isdigit() else retry_delay\n",
    "                        print(f\"‚ö†Ô∏è Rate limit atteint, attente {wait_time}s...\")\n",
    "                        await asyncio.sleep(wait_time)\n",
    "                        continue\n",
    "                    \n",
    "                    elif response.status == 401:\n",
    "                        raise ValueError(\"Token API invalide ou expir√©\")\n",
    "                    \n",
    "                    elif response.status >= 500:\n",
    "                        # Erreur serveur, on r√©essaie\n",
    "                        if attempt < MAX_RETRIES - 1:\n",
    "                            print(f\"‚ùå Erreur serveur (5xx), nouvelle tentative dans {retry_delay}s...\")\n",
    "                            await asyncio.sleep(retry_delay)\n",
    "                            retry_delay *= 2\n",
    "                            continue\n",
    "                        else:\n",
    "                            raise aiohttp.ClientError(\n",
    "                                f\"Erreur serveur persistante (status {response.status})\"\n",
    "                            )\n",
    "                    \n",
    "                    # V√©rification du statut\n",
    "                    response.raise_for_status()\n",
    "                    \n",
    "                    # Parsing de la r√©ponse\n",
    "                    results = await response.json()\n",
    "                    print(f\"‚úÖ R√©ponse re√ßue pour {os.path.basename(single_image_path)}\")\n",
    "                    \n",
    "                    # D√©lai entre les requ√™tes pour respecter le rate limit\n",
    "                    await asyncio.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "                    \n",
    "                    return results\n",
    "                    \n",
    "            except asyncio.TimeoutError:\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    print(f\"‚è±Ô∏è Timeout, nouvelle tentative...\")\n",
    "                    await asyncio.sleep(retry_delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise asyncio.TimeoutError(\n",
    "                        f\"Timeout apr√®s {MAX_RETRIES} tentatives pour '{single_image_path}'\"\n",
    "                    )\n",
    "            \n",
    "            except aiohttp.ClientError as e:\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    print(f\"‚ùå Erreur r√©seau, nouvelle tentative dans {retry_delay}s...\")\n",
    "                    await asyncio.sleep(retry_delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "        \n",
    "        raise Exception(f\"√âchec apr√®s {MAX_RETRIES} tentatives\")\n",
    "\n",
    "\n",
    "async def segment_clothes_batch(image_paths: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Segmente un lot d'images de mani√®re asynchrone avec rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: Liste des chemins d'images √† traiter\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Liste des r√©sultats de segmentation\n",
    "    \"\"\"\n",
    "    # S√©maphore pour limiter les requ√™tes concurrentes\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [\n",
    "            segment_clothes_async(session, img_path, semaphore)\n",
    "            for img_path in image_paths\n",
    "        ]\n",
    "        \n",
    "        # Ex√©cution de toutes les t√¢ches\n",
    "        results = await async_tqdm.gather(*tasks, desc=\"Segmentation\")        \n",
    "        \n",
    "        # Traitement des r√©sultats et des erreurs\n",
    "        processed_results = []\n",
    "        for i, result in enumerate(results):\n",
    "            if isinstance(result, Exception):\n",
    "                print(f\"‚ùå Erreur pour {image_paths[i]}: {result}\")\n",
    "                processed_results.append(None)\n",
    "            else:\n",
    "                processed_results.append(result)\n",
    "        \n",
    "        return processed_results\n",
    "\n",
    "\n",
    "# Fonction wrapper pour utiliser dans un notebook Jupyter\n",
    "def segment_images(image_paths: List[str]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fonction synchrone wrapper pour utilisation facile dans le notebook.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: Liste des chemins d'images (ou chemin unique)\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict] ou Dict: R√©sultats de la segmentation\n",
    "    \"\"\"\n",
    "    # Accepter un seul chemin ou une liste\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "    \n",
    "    print(f\"üöÄ D√©but de la segmentation de {len(image_paths)} image(s)...\")\n",
    "    print(f\"‚è±Ô∏è D√©lai entre requ√™tes: {DELAY_BETWEEN_REQUESTS:.2f}s\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Ex√©cution asynchrone\n",
    "    results = asyncio.run(segment_clothes_batch(image_paths))\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Traitement termin√© en {duration:.2f}s\")\n",
    "    print(f\"üìä Succ√®s: {sum(1 for r in results if r is not None)}/{len(image_paths)}\")\n",
    "    print(f\"‚ö° Vitesse moyenne: {len(image_paths)/duration:.2f} images/seconde\")\n",
    "\n",
    "    # Si une seule image, retourner directement le r√©sultat\n",
    "    if len(image_paths) == 1:\n",
    "        return results[0]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128f147",
   "metadata": {},
   "source": [
    "## 5. Fonctions Utilitaires pour le Traitement des Masques\n",
    "\n",
    "Le mod√®le que nous utilisons (`sayeed99/segformer_b3_clothes`) renvoie des masques pour diff√©rentes classes (cheveux, chapeau, etc.). Ces masques sont encod√©s en base64. Les fonctions ci-dessous sont fournies pour vous aider √† :\n",
    "1.  `CLASS_MAPPING`: Un dictionnaire qui associe les noms de classes (ex: \"Hat\") √† des identifiants num√©riques.\n",
    "2.  `get_image_dimensions`: R√©cup√©rer les dimensions d'une image.\n",
    "3.  `decode_base64_mask`: D√©coder un masque de base64 en une image (tableau NumPy) et le redimensionner.\n",
    "4.  `create_masks`: Combiner les masques de toutes les classes d√©tect√©es en un seul masque de segmentation final, o√π chaque pixel a la valeur de l'ID de sa classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_MAPPING = {\n",
    "    \"Background\": 0,\n",
    "    \"Hat\": 1,\n",
    "    \"Hair\": 2,\n",
    "    \"Sunglasses\": 3,\n",
    "    \"Upper-clothes\": 4,\n",
    "    \"Skirt\": 5,\n",
    "    \"Pants\": 6,\n",
    "    \"Dress\": 7,\n",
    "    \"Belt\": 8,\n",
    "    \"Left-shoe\": 9,\n",
    "    \"Right-shoe\": 10,\n",
    "    \"Face\": 11,\n",
    "    \"Left-leg\": 12,\n",
    "    \"Right-leg\": 13,\n",
    "    \"Left-arm\": 14,\n",
    "    \"Right-arm\": 15,\n",
    "    \"Bag\": 16,\n",
    "    \"Scarf\": 17\n",
    "}\n",
    "\n",
    "def get_image_dimensions(img_path):\n",
    "    \"\"\"\n",
    "    Get the dimensions of an image.\n",
    "\n",
    "    Args:\n",
    "        img_path (str): Path to the image.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (width, height) of the image.\n",
    "    \"\"\"\n",
    "    original_image = Image.open(img_path)\n",
    "    return original_image.size\n",
    "\n",
    "def decode_base64_mask(base64_string, width, height):\n",
    "    \"\"\"\n",
    "    Decode a base64-encoded mask into a NumPy array.\n",
    "\n",
    "    Args:\n",
    "        base64_string (str): Base64-encoded mask.\n",
    "        width (int): Target width.\n",
    "        height (int): Target height.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Single-channel mask array.\n",
    "    \"\"\"\n",
    "    mask_data = base64.b64decode(base64_string)\n",
    "    mask_image = Image.open(io.BytesIO(mask_data))\n",
    "    mask_array = np.array(mask_image)\n",
    "    if len(mask_array.shape) == 3:\n",
    "        mask_array = mask_array[:, :, 0]  # Take first channel if RGB\n",
    "    mask_image = Image.fromarray(mask_array).resize((width, height), Image.NEAREST)\n",
    "    return np.array(mask_image)\n",
    "\n",
    "def create_masks(results, width, height):\n",
    "    \"\"\"\n",
    "    Combine multiple class masks into a single segmentation mask.\n",
    "\n",
    "    Args:\n",
    "        results (list): List of dictionaries with 'label' and 'mask' keys.\n",
    "        width (int): Target width.\n",
    "        height (int): Target height.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Combined segmentation mask with class indices.\n",
    "    \"\"\"\n",
    "    combined_mask = np.zeros((height, width), dtype=np.uint8)  # Initialize with Background (0)\n",
    "\n",
    "    # Process non-Background masks first\n",
    "    for result in results:\n",
    "        label = result['label']\n",
    "        class_id = CLASS_MAPPING.get(label, 0)\n",
    "        if class_id == 0:  # Skip Background\n",
    "            continue\n",
    "        mask_array = decode_base64_mask(result['mask'], width, height)\n",
    "        combined_mask[mask_array > 0] = class_id\n",
    "\n",
    "    # Process Background last to ensure it doesn't overwrite other classes unnecessarily\n",
    "    # (Though the model usually provides non-overlapping masks for distinct classes other than background)\n",
    "    for result in results:\n",
    "        if result['label'] == 'Background':\n",
    "            mask_array = decode_base64_mask(result['mask'], width, height)\n",
    "            # Apply background only where no other class has been assigned yet\n",
    "            # This logic might need adjustment based on how the model defines 'Background'\n",
    "            # For this model, it seems safer to just let non-background overwrite it first.\n",
    "            # A simple application like this should be fine: if Background mask says pixel is BG, set it to 0.\n",
    "            # However, a more robust way might be to only set to background if combined_mask is still 0 (initial value)\n",
    "            combined_mask[mask_array > 0] = 0 # Class ID for Background is 0\n",
    "\n",
    "    return combined_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-2 (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
